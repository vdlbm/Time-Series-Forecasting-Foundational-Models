{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# My APIs\n",
    "import my_data_frequency_conversor as dfc\n",
    "import my_models as mm\n",
    "import my_prompting_tool as mpt \n",
    "\n",
    "# Classical models\n",
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import AutoARIMA\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing \n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.holtwinters import Holt\n",
    "\n",
    "# TimeSeries Transformer models\n",
    "## TimeGPT\n",
    "from nixtla import NixtlaClient\n",
    "\n",
    "## Moirai\n",
    "from uni2ts.eval_util.plot import plot_single\n",
    "from uni2ts.model.moirai import MoiraiForecast, MoiraiModule\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "from gluonts.dataset.split import split\n",
    "\n",
    "## Chronos\n",
    "from chronos import BaseChronosPipeline\n",
    "\n",
    "# Utilities\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()  # Carga el .env\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets local names\n",
    "DATA_HOURS = 'DATA/ETHUSD-H1.csv'\n",
    "DATA_DAYS = 'DATA/ETHUSD-D1.csv'\n",
    "DATA_MONTHS = 'DATA/ETHUSD-Monthly.csv'\n",
    "\n",
    "# Models initialization\n",
    "arima_h = mm.Model('ARIMA', 0, 0, 0)\n",
    "arima_d = mm.Model('ARIMA-d', 0, 0, 0)\n",
    "arima_m = mm.Model('ARIMA-m', 0, 0, 0)\n",
    "exp_smooth_h = mm.Model('EXPONENTIAL SMOOTHING', 0, 0, 0)\n",
    "exp_smooth_d = mm.Model('EXPONENTIAL SMOOTHING', 0, 0, 0)\n",
    "exp_smooth_m = mm.Model('EXPONENTIAL SMOOTHING', 0, 0, 0)\n",
    "holt_h = mm.Model('HOLT', 0, 0, 0)\n",
    "holt_d = mm.Model('HOLT', 0, 0, 0)\n",
    "holt_m = mm.Model('HOLT', 0, 0, 0)\n",
    "holt_winters_h = mm.Model('HOLT WINTERS', 0, 0, 0)\n",
    "holt_winters_d = mm.Model('HOLT WINTERS', 0, 0, 0)\n",
    "holt_winters_m = mm.Model('HOLT WINTERS', 0, 0, 0)\n",
    "timegpt_h = mm.Model('TIMEGPT', 0, 0, 0)\n",
    "timegpt_d = mm.Model('TIMEGPT', 0, 0, 0)\n",
    "timegpt_m = mm.Model('TIMEGPT', 0, 0, 0)\n",
    "lagllama_h = mm.Model('LAGLLAMA', 0, 0, 0)\n",
    "lagllama_d = mm.Model('LAGLLAMA', 0, 0, 0)\n",
    "lagllama_m = mm.Model('LAGLLAMA', 0, 0, 0)\n",
    "gpt_h = mm.Model('GPT-H', 0, 0, 0)\n",
    "gpt_d = mm.Model('GPT-D', 0, 0, 0)\n",
    "gpt_m = mm.Model('GPT-M', 0, 0, 0)\n",
    "\n",
    "# Models dictionary\n",
    "models_dict = {\n",
    "    'ARIMA_H': arima_h,\n",
    "    'ARIMA_D': arima_d,\n",
    "    'ARIMA_M': arima_m,\n",
    "    'EXP_SMOOTH_H': exp_smooth_h,\n",
    "    'EXP_SMOOTH_D': exp_smooth_d,\n",
    "    'EXP_SMOOTH_M': exp_smooth_m,\n",
    "    'HOLT_H': holt_h,\n",
    "    'HOLT_D': holt_d,\n",
    "    'HOLT_M': holt_m,\n",
    "    'HOLT_WINTERS_H': holt_winters_h,\n",
    "    'HOLT_WINTERS_D': holt_winters_d,\n",
    "    'HOLT_WINTERS_M': holt_winters_m,\n",
    "    'TIMEGPT_H': timegpt_h,\n",
    "    'TIMEGPT_D': timegpt_d,\n",
    "    'TIMEGPT_M': timegpt_m,\n",
    "    'LAGLLAMA_H': lagllama_h,\n",
    "    'LAGLLAMA_D': lagllama_d,\n",
    "    'LAGLLAMA_M': lagllama_m,\n",
    "    'GPT_H': gpt_h,\n",
    "    'GPT_D': gpt_d,\n",
    "    'GPT_M': gpt_m\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data monthly frequency preprocessing\n",
    "data_months = pd.read_csv(DATA_MONTHS, delimiter=',')\n",
    "train_data_months, val_data_months = dfc.split_data(data_months)\n",
    "train_data_months = train_data_months.asfreq('MS')\n",
    "val_data_months = val_data_months.asfreq('MS')\n",
    "\n",
    "# Data daily frequency preprocessing\n",
    "data_days = pd.read_csv(DATA_DAYS, delimiter=',')\n",
    "train_data_days, val_data_days = dfc.split_data(data_days)\n",
    "train_data_days = train_data_days.asfreq('d')\n",
    "val_data_days = val_data_days.asfreq('d')\n",
    "\n",
    "# Data hourly frequency preprocessing\n",
    "data_hours = pd.read_csv(DATA_HOURS, delimiter=',')\n",
    "train_data_hours, val_data_hours = dfc.split_data(data_hours)\n",
    "train_data_hours = train_data_hours.asfreq('h')\n",
    "val_data_hours = val_data_hours.asfreq('h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ds            y unique_id\n",
      "0 2021-01-01   736.220000     Value\n",
      "1 2021-02-01  1313.868608     Value\n",
      "2 2021-03-01  1424.400000     Value\n",
      "3 2021-04-01  1919.300000     Value\n",
      "4 2021-05-01  2769.400000     Value\n",
      "   unique_id         ds    AutoARIMA  AutoARIMA-lo-95  AutoARIMA-hi-95\n",
      "0      Value 2024-01-01  2041.977347      1060.637520      3023.317174\n",
      "1      Value 2024-02-01  2032.962850       750.906860      3315.018840\n",
      "2      Value 2024-03-01  2025.384395       567.741214      3483.027576\n",
      "3      Value 2024-04-01  2019.013215       449.070571      3588.955858\n",
      "4      Value 2024-05-01  2013.656986       368.962716      3658.351255\n",
      "5      Value 2024-06-01  2009.154023       313.614072      3704.693975\n",
      "6      Value 2024-07-01  2005.368399       274.793017      3735.943781\n",
      "7      Value 2024-08-01  2002.185838       247.270269      3757.101406\n",
      "8      Value 2024-09-01  1999.510270       227.593427      3771.427112\n",
      "9      Value 2024-10-01  1997.260929       213.425816      3781.096043\n",
      "10     Value 2024-11-01  1995.369917       203.159123      3787.580710\n",
      "11     Value 2024-12-01  1993.780149       195.673203      3791.887095\n"
     ]
    }
   ],
   "source": [
    "# Convertir la serie en DataFrame con las columnas correctas\n",
    "arima_m.arima_format = dfc.convert_to_nixtla(train_data_months)\n",
    "arima_m.model = StatsForecast(\n",
    "    models=[AutoARIMA(season_length=1)],\n",
    "    freq='MS'\n",
    ")\n",
    "\n",
    "arima_m.model.fit(arima_m.arima_format)\n",
    "arima_m.forecast = arima_m.model.predict(h=12, level=[95])\n",
    "print(arima_m.forecast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get actual values and predictions as np arrays\n",
    "y_true = val_data_months.get('y').values\n",
    "y_pred = arima_m.forecast.get('AutoARIMA').values\n",
    "\n",
    "# Calculate metrics\n",
    "arima_m.mse = mean_squared_error(y_true, y_pred)\n",
    "arima_m.mae = mean_absolute_error(y_true, y_pred)\n",
    "arima_m.mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "\n",
    "print(f\"ARIMA - MSE: {arima_m.mse}, MAE: {arima_m.mae}, MAPE: {arima_m.mape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ARIMA forecast against the actual data with the confidence intervals\n",
    "train_df = train_data_months.reset_index().iloc[:, :2]\n",
    "train_df.columns = ['ds', 'y']\n",
    "train_df['type'] = 'Train'\n",
    "\n",
    "val_df = val_data_months.reset_index().iloc[:, :2]\n",
    "val_df.columns = ['ds', 'y']\n",
    "val_df['type'] = 'Validation'\n",
    "\n",
    "# Concatenar los datos de entrenamiento y validación para graficar\n",
    "all_data = pd.concat([train_df, val_df])\n",
    "\n",
    "# Graficar los datos de entrenamiento y validación\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(all_data['ds'], all_data['y'], label='Datos Reales', color='blue')\n",
    "\n",
    "# Graficar las predicciones\n",
    "plt.plot(arima_m.forecast['ds'], arima_m.forecast['AutoARIMA'], label='Predicciones', color='red')\n",
    "\n",
    "# Graficar el intervalo de confianza\n",
    "plt.fill_between(\n",
    "    arima_m.forecast['ds'],\n",
    "    arima_m.forecast['AutoARIMA-lo-95'],\n",
    "    arima_m.forecast['AutoARIMA-hi-95'],\n",
    "    color='pink',\n",
    "    alpha=0.3,\n",
    "    label='Intervalo de Confianza 95%'\n",
    ")\n",
    "\n",
    "# Añadir leyenda y etiquetas\n",
    "plt.legend()\n",
    "plt.title('Predicciones vs Datos Reales')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Open')\n",
    "\n",
    "# Mostrar la gráfica\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_d.nixtla_format = dfc.convert_to_nixtla(train_data_days)\n",
    "print(arima_d.nixtla_format.head())\n",
    "arima_d.model = StatsForecast(\n",
    "    models=[AutoARIMA(season_length=1)],\n",
    "    freq='D'\n",
    ")\n",
    "'''\n",
    "season_length=7:Se asume que hay un patrón semanal en los datos diarios. \n",
    "Si no es el caso, puedes cambiarlo a 1 (sin estacionalidad) o 365 (estacionalidad anual).\n",
    "'''\n",
    "arima_d.model.fit(arima_d.nixtla_format)\n",
    "arima_d.forecast = arima_d.model.predict(h=366, level=[95])\n",
    "print(arima_d.forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get actual values and predictions as np arrays\n",
    "y_true = val_data_days.get('y').values\n",
    "y_pred = arima_d.forecast.get('AutoARIMA').values\n",
    "\n",
    "# Calculate metrics\n",
    "arima_d.mse = mean_squared_error(y_true, y_pred)\n",
    "arima_d.mae = mean_absolute_error(y_true, y_pred)\n",
    "arima_d.mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "\n",
    "print(f\"ARIMA - MSE: {arima_d.mse}, MAE: {arima_d.mae}, MAPE: {arima_d.mape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ARIMA forecast against the actual data with the confidence intervals\n",
    "train_df = train_data_days.reset_index().iloc[:, :2]  # Solo tomar las dos primeras columnas\n",
    "train_df.columns = ['ds', 'y']\n",
    "train_df['type'] = 'Train'\n",
    "\n",
    "val_df = val_data_days.reset_index().iloc[:, :2]  # Solo tomar las dos primeras columnas\n",
    "val_df.columns = ['ds', 'y']  # Renombrar columnas\n",
    "val_df['type'] = 'Validation'\n",
    "\n",
    "# Concatenar los datos de entrenamiento y validación para graficar\n",
    "all_data = pd.concat([train_df, val_df])\n",
    "\n",
    "# Graficar los datos de entrenamiento y validación\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(all_data['ds'], all_data['y'], label='Datos Reales', color='blue')\n",
    "\n",
    "# Graficar las predicciones\n",
    "plt.plot(arima_d.forecast['ds'], arima_d.forecast['AutoARIMA'], label='Predicciones', color='red')\n",
    "\n",
    "# Graficar el intervalo de confianza\n",
    "plt.fill_between(\n",
    "    arima_d.forecast['ds'],\n",
    "    arima_d.forecast['AutoARIMA-lo-95'],\n",
    "    arima_d.forecast['AutoARIMA-hi-95'],\n",
    "    color='pink',\n",
    "    alpha=0.3,\n",
    "    label='Intervalo de Confianza 95%'\n",
    ")\n",
    "\n",
    "# Añadir leyenda y etiquetas\n",
    "plt.legend()\n",
    "plt.title('Predicciones vs Datos Reales')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Open')\n",
    "\n",
    "# Mostrar la gráfica\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_h.nixtla_format = dfc.convert_to_nixtla(train_data_hours)\n",
    "print(arima_h.nixtla_format.head())\n",
    "arima_h.model = StatsForecast(\n",
    "    models=[AutoARIMA(season_length=24)],\n",
    "    freq='H'\n",
    ")\n",
    "'''\n",
    "season_length=24: Se asume que hay un patrón diario en los datos horarios.\n",
    "Si no es el caso, puedes cambiarlo a 1 (sin estacionalidad) o 168 (estacionalidad semanal).\n",
    "'''\n",
    "arima_h.model.fit(arima_h.nixtla_format)\n",
    "arima_h.forecast = arima_h.model.predict(h=24*366, level=[95])\n",
    "print(arima_h.forecast.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ARIMA forecast against the actual data with the confidence intervals\n",
    "train_df = train_data_hours.reset_index().iloc[:, :2]  # Solo tomar las dos primeras columnas\n",
    "train_df.columns = ['ds', 'y']\n",
    "train_df['type'] = 'Train'\n",
    "\n",
    "val_df = val_data_hours.reset_index().iloc[:, :2]  # Solo tomar las dos primeras columnas\n",
    "val_df.columns = ['ds', 'y']  # Renombrar columnas\n",
    "val_df['type'] = 'Validation'\n",
    "\n",
    "# Concatenar los datos de entrenamiento y validación para graficar\n",
    "all_data = pd.concat([train_df, val_df])\n",
    "\n",
    "# Graficar los datos de entrenamiento y validación\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(all_data['ds'], all_data['y'], label='Datos Reales', color='blue')\n",
    "\n",
    "# Graficar las predicciones\n",
    "plt.plot(arima_h.forecast['ds'], arima_h.forecast['AutoARIMA'], label='Predicciones', color='red')\n",
    "\n",
    "# Graficar el intervalo de confianza\n",
    "plt.fill_between(\n",
    "    arima_h.forecast['ds'],\n",
    "    arima_h.forecast['AutoARIMA-lo-95'],\n",
    "    arima_h.forecast['AutoARIMA-hi-95'],\n",
    "    color='pink',\n",
    "    alpha=0.3,\n",
    "    label='Intervalo de Confianza 95%'\n",
    ")\n",
    "\n",
    "# Añadir leyenda y etiquetas\n",
    "plt.legend()\n",
    "plt.title('Predicciones vs Datos Reales')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Open')\n",
    "\n",
    "# Mostrar la gráfica\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar el modelo con cálculo óptimo de alpha\n",
    "exp_smooth_m.model = SimpleExpSmoothing(train_data_months).fit(optimized=True)\n",
    "\n",
    "# Obtener el alpha óptimo\n",
    "exp_smooth_m.optimal_alpha = exp_smooth_m.model.model.params[\"smoothing_level\"]\n",
    "print(f\"Alpha óptimo: {exp_smooth_m.optimal_alpha:.4f}\")\n",
    "\n",
    "# Predicción a futuro (próximo año)\n",
    "exp_smooth_m.forecast = exp_smooth_m.model.forecast(steps=12)\n",
    "exp_smooth_m.forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get actual values and predictions as np arrays\n",
    "y_true = val_data_months.get('y').values\n",
    "y_pred = exp_smooth_m.forecast.values\n",
    "\n",
    "# Calculate metrics\n",
    "exp_smooth_m.mse = mean_squared_error(y_true, y_pred)\n",
    "exp_smooth_m.mae = mean_absolute_error(y_true, y_pred)\n",
    "exp_smooth_m.mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "\n",
    "print(f\"Exponential Smoothing - MSE: {exp_smooth_m.mse}, MAE: {exp_smooth_m.mae}, MAPE: {exp_smooth_m.mape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar el modelo con cálculo óptimo de alpha\n",
    "exp_smooth_d.model = SimpleExpSmoothing(train_data_days).fit(optimized=True)\n",
    "\n",
    "# Obtener el alpha óptimo\n",
    "exp_smooth_d.optimal_alpha = exp_smooth_d.model.model.params[\"smoothing_level\"]\n",
    "print(f\"Alpha óptimo: {exp_smooth_d.optimal_alpha:.4f}\")\n",
    "\n",
    "# Predicción a futuro (próximo año)\n",
    "exp_smooth_d.forecast = exp_smooth_d.model.forecast(steps=366)\n",
    "exp_smooth_d.forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar el modelo con cálculo óptimo de alpha\n",
    "exp_smooth_h.model = SimpleExpSmoothing(train_data_hours).fit(optimized=True)\n",
    "\n",
    "# Obtener el alpha óptimo\n",
    "exp_smooth_h.optimal_alpha = exp_smooth_h.model.model.params[\"smoothing_level\"]\n",
    "print(f\"Alpha óptimo: {exp_smooth_h.optimal_alpha:.4f}\")\n",
    "\n",
    "# Predicción a futuro (próximo año)\n",
    "exp_smooth_h.forecast = exp_smooth_h.model.forecast(steps=366*24)\n",
    "exp_smooth_h.forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holt_m.model = Holt(train_data_months).fit(optimized=True)\n",
    "holt_m.optimal_alpha = holt_m.model.model.params[\"smoothing_level\"]\n",
    "holt_m.optimal_beta = holt_m.model.model.params[\"smoothing_trend\"]\n",
    "print(f\"Alpha óptimo: {holt_m.optimal_alpha:.4f}\")\n",
    "print(f\"Beta óptimo: {holt_m.optimal_beta:.4f}\")\n",
    "holt_m.forecast = holt_m.model.forecast(steps=12)\n",
    "holt_m.forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get actual values and predictions as np arrays\n",
    "y_true = val_data_months.get('y').values\n",
    "y_pred = holt_m.forecast.values\n",
    "\n",
    "# Calculate metrics\n",
    "holt_m.mse = mean_squared_error(y_true, y_pred)\n",
    "holt_m.mae = mean_absolute_error(y_true, y_pred)\n",
    "holt_m.mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "\n",
    "print(f\"Holt Winter - MSE: {holt_m.mse}, MAE: {holt_m.mae}, MAPE: {holt_m.mape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holt_d.model = Holt(train_data_days).fit(optimized=True)\n",
    "holt_d.optimal_alpha = holt_d.model.model.params[\"smoothing_level\"]\n",
    "holt_d.optimal_beta = holt_d.model.model.params[\"smoothing_trend\"]\n",
    "print(f\"Alpha óptimo: {holt_d.optimal_alpha:.4f}\")\n",
    "print(f\"Beta óptimo: {holt_d.optimal_beta:.4f}\")\n",
    "holt_d.forecast = holt_d.model.forecast(steps=366)\n",
    "holt_d.forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holt_h.model = Holt(train_data_hours).fit(optimized=True)\n",
    "holt_h.optimal_alpha = holt_h.model.model.params[\"smoothing_level\"]\n",
    "holt_h.optimal_beta = holt_h.model.model.params[\"smoothing_trend\"]\n",
    "print(f\"Alpha óptimo: {holt_h.optimal_alpha:.4f}\")\n",
    "print(f\"Beta óptimo: {holt_h.optimal_beta:.4f}\")\n",
    "holt_h.forecast = holt_h.model.forecast(steps=366*24)\n",
    "holt_h.forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar el modelo con cálculo óptimo de alpha\n",
    "holt_winters_m.model = ExponentialSmoothing(train_data_months, trend='add', seasonal='add', seasonal_periods=12).fit(optimized=True)\n",
    "\n",
    "# Obtener el alpha óptimo\n",
    "holt_winters_m.optimal_params = holt_winters_m.model.params\n",
    "print(f\"Parámetros óptimos: {holt_winters_m.optimal_params}\")\n",
    "\n",
    "# Predicción a futuro (próximo año)\n",
    "holt_winters_m.forecast = holt_winters_m.model.forecast(steps=12)\n",
    "holt_winters_m.forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get actual values and predictions as np arrays\n",
    "y_true = val_data_months.get('y').values\n",
    "y_pred = holt_winters_m.forecast.values\n",
    "\n",
    "# Calculate metrics\n",
    "holt_winters_m.mse = mean_squared_error(y_true, y_pred)\n",
    "holt_winters_m.mae = mean_absolute_error(y_true, y_pred)\n",
    "holt_winters_m.mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "\n",
    "print(f\"Holt Winter - MSE: {holt_winters_m.mse}, MAE: {holt_winters_m.mae}, MAPE: {holt_winters_m.mape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar el modelo con cálculo óptimo de alpha, seasonal period de 7 para intentar calcular una ciclidad semanal\n",
    "holt_winters_d.model = ExponentialSmoothing(train_data_days, trend='add', seasonal='add', seasonal_periods=7).fit(optimized=True)\n",
    "\n",
    "# Obtener el alpha óptimo\n",
    "holt_winters_d.optimal_params = holt_winters_d.model.params\n",
    "print(f\"Parámetros óptimos: {holt_winters_d.optimal_params}\")\n",
    "\n",
    "# Predicción a futuro (próximo año)\n",
    "holt_winters_d.forecast = holt_winters_d.model.forecast(steps=366)\n",
    "holt_winters_d.forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar el modelo con cálculo óptimo de alpha, seasonal period de 24 para intentar calcular una ciclidad diaria\n",
    "holt_winters_h.model = ExponentialSmoothing(train_data_hours, trend='add', seasonal='add', seasonal_periods=24).fit(optimized=True)\n",
    "\n",
    "# Obtener el alpha óptimo\n",
    "holt_winters_h.optimal_params = holt_winters_h.model.params\n",
    "print(f\"Parámetros óptimos: {holt_winters_h.optimal_params}\")\n",
    "\n",
    "# Predicción a futuro (próximo año)\n",
    "holt_winters_h.forecast = holt_winters_h.model.forecast(steps=366*24)\n",
    "holt_winters_h.forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nixtla.nixtla_client:Happy Forecasting! :)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TimeGPT Api Key and Client validation\n",
    "my_api_key = os.getenv(\"MY_NIXTLA_API_KEY\")\n",
    "nixtla_client = NixtlaClient(\n",
    "    api_key = my_api_key\n",
    ")\n",
    "nixtla_client.validate_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nixtla.nixtla_client:Validating inputs...\n",
      "INFO:nixtla.nixtla_client:Inferred freq: MS\n",
      "INFO:nixtla.nixtla_client:Preprocessing dataframes...\n",
      "INFO:nixtla.nixtla_client:Querying model metadata...\n",
      "INFO:nixtla.nixtla_client:Restricting input...\n",
      "INFO:nixtla.nixtla_client:Calling Forecast Endpoint...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"timegpt_d.predictions = nixtla_client.predict(\\n    df=train_data_days,\\n    horizon=366,\\n    time_col='DateTime',\\n    target_col='Open',\\n)\\n\\ntimegpt_h.predictions = nixtla_client.predict(\\n    df=train_data_hours,\\n    horizon=366*24,\\n    time_col='DateTime',\\n    target_col='Open',\\n)\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "timegpt_m.predictions = nixtla_client.forecast(\n",
    "    df=train_data_months,\n",
    "    h=12,\n",
    "    time_col='DateTime',\n",
    "    target_col='y',\n",
    ")\n",
    "\n",
    "'''timegpt_d.predictions = nixtla_client.predict(\n",
    "    df=train_data_days,\n",
    "    horizon=366,\n",
    "    time_col='DateTime',\n",
    "    target_col='Open',\n",
    ")\n",
    "\n",
    "timegpt_h.predictions = nixtla_client.predict(\n",
    "    df=train_data_hours,\n",
    "    horizon=366*24,\n",
    "    time_col='DateTime',\n",
    "    target_col='Open',\n",
    ")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time GPT - MSE: 1203824.7001717165, MAE: 958.0057166666666, MAPE: 0.293421838215491\n"
     ]
    }
   ],
   "source": [
    "# Get actual values and predictions as np arrays\n",
    "y_true = val_data_months.get('y').values\n",
    "y_pred = timegpt_m.predictions.get('TimeGPT').values\n",
    "\n",
    "# Calculate metrics\n",
    "timegpt_m.mse = mean_squared_error(y_true, y_pred)\n",
    "timegpt_m.mae = mean_absolute_error(y_true, y_pred)\n",
    "timegpt_m.mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "\n",
    "print(f\"Time GPT - MSE: {timegpt_m.mse}, MAE: {timegpt_m.mae}, MAPE: {timegpt_m.mape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/SalesforceAIResearch/uni2ts/\n",
    "%pip install uni2ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Need at least 3 dates to infer frequency",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m data_months \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(DATA_MONTHS, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m df \u001b[38;5;241m=\u001b[39m dfc\u001b[38;5;241m.\u001b[39mconvert_to_moirai(data_months)\n\u001b[1;32m---> 12\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mPandasDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Split into train/test set\u001b[39;00m\n\u001b[0;32m     15\u001b[0m train, test_template \u001b[38;5;241m=\u001b[39m split(\n\u001b[0;32m     16\u001b[0m     ds, offset\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mTEST\n\u001b[0;32m     17\u001b[0m )  \u001b[38;5;66;03m# assign last TEST time steps as test set\u001b[39;00m\n",
      "File \u001b[1;32m<string>:12\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, dataframes, target, feat_dynamic_real, past_feat_dynamic_real, timestamp, freq, static_features, future_length, unchecked, assume_sorted, dtype)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gluonts\\dataset\\pandas.py:119\u001b[0m, in \u001b[0;36mPandasDataset.__post_init__\u001b[1;34m(self, dataframes, static_features)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfreq \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    116\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestamp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to provide `freq` along with `timestamp`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfreq \u001b[38;5;241m=\u001b[39m \u001b[43minfer_freq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m static_features \u001b[38;5;241m=\u001b[39m maybe\u001b[38;5;241m.\u001b[39munwrap_or_else(static_features, pd\u001b[38;5;241m.\u001b[39mDataFrame)\n\u001b[0;32m    123\u001b[0m object_columns \u001b[38;5;241m=\u001b[39m static_features\u001b[38;5;241m.\u001b[39mselect_dtypes(\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    125\u001b[0m )\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gluonts\\dataset\\pandas.py:331\u001b[0m, in \u001b[0;36minfer_freq\u001b[1;34m(index)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(index, pd\u001b[38;5;241m.\u001b[39mPeriodIndex):\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m index\u001b[38;5;241m.\u001b[39mfreqstr\n\u001b[1;32m--> 331\u001b[0m freq \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_freq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;66;03m# pandas likes to infer the `start of x` frequency, however when doing\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;66;03m# df.to_period(\"<x>S\"), it fails, so we avoid using it. It's enough to\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;66;03m# remove the trailing S, e.g `MS` -> `M\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(freq) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m freq\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\tseries\\frequencies.py:180\u001b[0m, in \u001b[0;36minfer_freq\u001b[1;34m(index)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(index, DatetimeIndex):\n\u001b[0;32m    178\u001b[0m     index \u001b[38;5;241m=\u001b[39m DatetimeIndex(index)\n\u001b[1;32m--> 180\u001b[0m inferer \u001b[38;5;241m=\u001b[39m \u001b[43m_FrequencyInferer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inferer\u001b[38;5;241m.\u001b[39mget_freq()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\tseries\\frequencies.py:214\u001b[0m, in \u001b[0;36m_FrequencyInferer.__init__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi8values \u001b[38;5;241m=\u001b[39m tz_convert_from_utc(\n\u001b[0;32m    210\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi8values, index\u001b[38;5;241m.\u001b[39mtz, reso\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_creso\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(index) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m--> 214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeed at least 3 dates to infer frequency\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_monotonic \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_is_monotonic_increasing \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_is_monotonic_decreasing\n\u001b[0;32m    218\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Need at least 3 dates to infer frequency"
     ]
    }
   ],
   "source": [
    "SIZE = \"small\"  # model size: choose from {'small', 'base', 'large'}\n",
    "PDT = 12  # prediction length: any positive integer\n",
    "CTX = 12*3  # context length: any positive integer\n",
    "PSZ = \"auto\"  # patch size: choose from {\"auto\", 8, 16, 32, 64, 128}\n",
    "BSZ = 32  # batch size: any positive integer\n",
    "TEST = 12  # test set length: any positive integer\n",
    "\n",
    "# Convert into GluonTS dataset\n",
    "data_months = pd.read_csv(DATA_MONTHS, delimiter=',')\n",
    "df = dfc.convert_to_moirai(data_months)\n",
    "\n",
    "ds = PandasDataset(dict(df))\n",
    "\n",
    "# Split into train/test set\n",
    "train, test_template = split(\n",
    "    ds, offset=-TEST\n",
    ")  # assign last TEST time steps as test set\n",
    "\n",
    "\n",
    "\n",
    "# Construct rolling window evaluation\n",
    "test_data = test_template.generate_instances(\n",
    "    prediction_length=PDT,  # number of time steps for each prediction\n",
    "    windows=TEST // PDT,  # number of windows in rolling window evaluation\n",
    "    distance=PDT,  # number of time steps between each window - distance=PDT for non-overlapping windows\n",
    ")\n",
    "\n",
    "# Prepare pre-trained model by downloading model weights from huggingface hub\n",
    "model = MoiraiForecast(\n",
    "    module=MoiraiModule.from_pretrained(f\"Salesforce/moirai-1.0-R-{SIZE}\"),\n",
    "    prediction_length=PDT,\n",
    "    context_length=CTX,\n",
    "    patch_size=PSZ,\n",
    "    num_samples=100,\n",
    "    target_dim=1,\n",
    "    feat_dynamic_real_dim=ds.num_feat_dynamic_real,\n",
    "    past_feat_dynamic_real_dim=ds.num_past_feat_dynamic_real,\n",
    ")\n",
    "\n",
    "predictor = model.create_predictor(batch_size=BSZ)\n",
    "forecasts = predictor.predict(test_data.input)\n",
    "\n",
    "input_it = iter(test_data.input)\n",
    "label_it = iter(test_data.label)\n",
    "forecast_it = iter(forecasts)\n",
    "\n",
    "inp = next(input_it)\n",
    "label = next(label_it)\n",
    "forecast = next(forecast_it)\n",
    "\n",
    "plot_single(\n",
    "    inp, \n",
    "    label, \n",
    "    forecast, \n",
    "    context_length=200,\n",
    "    name=\"pred\",\n",
    "    show_label=True,\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la serie temporal a lenguaje natural\n",
    "desp = mpt.paraphrase_initial(gpt_m.name)\n",
    "train_lan = mpt.paraphrase_seq2lan(train_data_months, desp)\n",
    "\n",
    "# Realizar la predicción para los próximos 12 meses (1 año)\n",
    "steps = 12  # Número de meses a predecir\n",
    "model_name = 'gpt-4o-mini'  \n",
    "predicted_lan = mpt.paraphrasing_predict_llm(desp, train_lan, steps, model_name)\n",
    "print(predicted_lan)\n",
    "# Convertir la predicción en lenguaje natural de vuelta a una serie temporal\n",
    "predicted_series = mpt.recover_lan2seq(predicted_lan)\n",
    "\n",
    "# Mostrar la serie temporal predicha\n",
    "print(predicted_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "#login(token=HF_TOKEN)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float32,   # usar float32 para CPU\n",
    "    device_map=\"cpu\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(\"¿Qué es el aprendizaje automático?\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
