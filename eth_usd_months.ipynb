{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vicde\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Librerías generales\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Mis propias librerías\n",
    "import my_data_frequency_conversor as dfc\n",
    "import my_models as mm\n",
    "import my_llm_ts_preprocessor as tsp\n",
    "import my_prompting_tool as mpt \n",
    "\n",
    "# Librerías de los métodos tradicionales\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing \n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.holtwinters import Holt\n",
    "\n",
    "# Creación  y carga de archivo .env, para gestionar las API KEYS\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()  # Carga el .env\n",
    "\n",
    "# Librerías de Nixtla\n",
    "from nixtla import NixtlaClient\n",
    "\n",
    "# Librerías de PyTorch\n",
    "import torch\n",
    "\n",
    "# Librerías de Moirai\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "from gluonts.dataset.split import split\n",
    "from uni2ts.eval_util.plot import plot_single\n",
    "from uni2ts.model.moirai import MoiraiForecast\n",
    "from uni2ts.model.moirai import MoiraiModule\n",
    "\n",
    "# Librerías de Chronos\n",
    "from chronos import BaseChronosPipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA_HOURS = 'DATA/ETHUSD-H1.csv'\n",
    "#DATA_DAYS = 'DATA/ETHUSD-D1.csv'\n",
    "DATA_MONTHS = 'DATA/ETHUSD-Monthly.csv'\n",
    "#arima_h = mm.Model('ARIMA')\n",
    "#arima_d = mm.Model('ARIMA-d')\n",
    "arima_m = mm.Model('ARIMA-m')\n",
    "#exp_smooth_h = mm.Model('EXPONENTIAL SMOOTHING')\n",
    "#exp_smooth_d = mm.Model('EXPONENTIAL SMOOTHING')\n",
    "exp_smooth_m = mm.Model('EXPONENTIAL SMOOTHING')\n",
    "#holt_h = mm.Model('HOLT')\n",
    "#holt_d = mm.Model('HOLT')\n",
    "holt_m = mm.Model('HOLT')\n",
    "#holt_winters_h = mm.Model('HOLT WINTERS')\n",
    "#holt_winters_d = mm.Model('HOLT WINTERS')\n",
    "holt_winters_m = mm.Model('HOLT WINTERS')\n",
    "#timegpt_h = mm.Model('TIMEGPT')\n",
    "#timegpt_d = mm.Model('TIMEGPT')\n",
    "timegpt_m = mm.Model('TIMEGPT')\n",
    "#lagllama_h = mm.Model('LAGLLAMA')\n",
    "#lagllama_d = mm.Model('LAGLLAMA')\n",
    "lagllama_m = mm.Model('LAGLLAMA')\n",
    "#gpt_h = mm.Model('GPT-H')\n",
    "#gpt_d = mm.Model('GPT-D')\n",
    "gpt_m = mm.Model('GPT-M')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      y\n",
      "DateTime               \n",
      "2021-01-01   736.220000\n",
      "2021-02-01  1313.868608\n",
      "2021-03-01  1424.400000\n",
      "2021-04-01  1919.300000\n",
      "2021-05-01  2769.400000\n",
      "                 y\n",
      "DateTime          \n",
      "2024-01-01  2286.5\n",
      "2024-02-01  2283.0\n",
      "2024-03-01  3339.2\n",
      "2024-04-01  3650.6\n",
      "2024-05-01  3020.3\n"
     ]
    }
   ],
   "source": [
    "data_months = pd.read_csv(DATA_MONTHS, delimiter=',')\n",
    "train_data_months, val_data_months = dfc.split_data(data_months)\n",
    "train_data_months = train_data_months.asfreq('MS')\n",
    "val_data_months = val_data_months.asfreq('MS')\n",
    "print(train_data_months.head())\n",
    "print(val_data_months.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import AutoARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la serie en DataFrame con las columnas correctas\n",
    "arima_m.nixtla_format = dfc.convert_to_nixtla(train_data_months)\n",
    "print(arima_m.nixtla_format.head())\n",
    "arima_m.model = StatsForecast(\n",
    "    models=[AutoARIMA(season_length=1)],\n",
    "    freq='MS'\n",
    ")\n",
    "\n",
    "arima_m.model.fit(arima_m.nixtla_format)\n",
    "arima_m.forecast = arima_m.model.predict(h=12, level=[95])\n",
    "print(arima_m.forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ARIMA forecast against the actual data with the confidence intervals\n",
    "train_df = train_data_months.reset_index().iloc[:, :2]  # Solo tomar las dos primeras columnas\n",
    "train_df.columns = ['ds', 'y']\n",
    "train_df['type'] = 'Train'\n",
    "\n",
    "val_df = val_data_months.reset_index().iloc[:, :2]  # Solo tomar las dos primeras columnas\n",
    "val_df.columns = ['ds', 'y']  # Renombrar columnas\n",
    "val_df['type'] = 'Validation'\n",
    "\n",
    "# Concatenar los datos de entrenamiento y validación para graficar\n",
    "all_data = pd.concat([train_df, val_df])\n",
    "\n",
    "# Graficar los datos de entrenamiento y validación\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(all_data['ds'], all_data['y'], label='Datos Reales', color='blue')\n",
    "\n",
    "# Graficar las predicciones\n",
    "plt.plot(arima_m.forecast['ds'], arima_m.forecast['AutoARIMA'], label='Predicciones', color='red')\n",
    "\n",
    "# Graficar el intervalo de confianza\n",
    "plt.fill_between(\n",
    "    arima_m.forecast['ds'],\n",
    "    arima_m.forecast['AutoARIMA-lo-95'],\n",
    "    arima_m.forecast['AutoARIMA-hi-95'],\n",
    "    color='pink',\n",
    "    alpha=0.3,\n",
    "    label='Intervalo de Confianza 95%'\n",
    ")\n",
    "\n",
    "# Añadir leyenda y etiquetas\n",
    "plt.legend()\n",
    "plt.title('Predicciones vs Datos Reales')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Eth(USD)')\n",
    "\n",
    "# Mostrar la gráfica\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar el modelo con cálculo óptimo de alpha\n",
    "exp_smooth_m.model = SimpleExpSmoothing(train_data_months).fit(optimized=True)\n",
    "\n",
    "# Obtener el alpha óptimo\n",
    "exp_smooth_m.optimal_alpha = exp_smooth_m.model.model.params[\"smoothing_level\"]\n",
    "print(f\"Alpha óptimo: {exp_smooth_m.optimal_alpha:.4f}\")\n",
    "\n",
    "# Predicción a futuro (próximo año)\n",
    "exp_smooth_m.forecast = exp_smooth_m.model.forecast(steps=12)\n",
    "exp_smooth_m.forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holt_m.model = Holt(train_data_months).fit(optimized=True)\n",
    "holt_m.optimal_alpha = holt_m.model.model.params[\"smoothing_level\"]\n",
    "holt_m.optimal_beta = holt_m.model.model.params[\"smoothing_trend\"]\n",
    "print(f\"Alpha óptimo: {holt_m.optimal_alpha:.4f}\")\n",
    "print(f\"Beta óptimo: {holt_m.optimal_beta:.4f}\")\n",
    "holt_m.forecast = holt_m.model.forecast(steps=12)\n",
    "holt_m.forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar el modelo con cálculo óptimo de alpha\n",
    "holt_winters_m.model = ExponentialSmoothing(train_data_months, trend='add', seasonal='add', seasonal_periods=12).fit(optimized=True)\n",
    "\n",
    "# Obtener el alpha óptimo\n",
    "holt_winters_m.optimal_params = holt_winters_m.model.params\n",
    "print(f\"Parámetros óptimos: {holt_winters_m.optimal_params}\")\n",
    "\n",
    "# Predicción a futuro (próximo año)\n",
    "holt_winters_m.forecast = holt_winters_m.model.forecast(steps=12)\n",
    "holt_winters_m.forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_months_nixtla = dfc.convert_to_nixtla(train_data_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_api_key = os.getenv(\"MY_NIXTLA_API_KEY\")\n",
    "nixtla_client = NixtlaClient(\n",
    "    api_key = my_api_key\n",
    ")\n",
    "\n",
    "\n",
    "#nixtla_client.plot(train_data_months_nixtla, timegpt_m.predictions)\n",
    "\n",
    "'''timegpt_d.predictions = nixtla_client.predict(\n",
    "    df=train_data_days,\n",
    "    horizon=366,\n",
    "    time_col='DateTime',\n",
    "    target_col='Open',\n",
    ")\n",
    "\n",
    "timegpt_h.predictions = nixtla_client.predict(\n",
    "    df=train_data_hours,\n",
    "    horizon=366*24,\n",
    "    time_col='DateTime',\n",
    "    target_col='Open',\n",
    ")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = \"small\"  # model size: choose from {'small', 'base', 'large'}\n",
    "PDT = 12  # prediction length: any positive integer\n",
    "CTX = 12*3  # context length: any positive integer\n",
    "PSZ = \"auto\"  # patch size: choose from {\"auto\", 8, 16, 32, 64, 128}\n",
    "BSZ = 32  # batch size: any positive integer\n",
    "TEST = 12  # test set length: any positive integer\n",
    "\n",
    "# Convert into GluonTS dataset\n",
    "df = dfc.convert_to_moirai(data_months)\n",
    "\n",
    "ds = PandasDataset(dict(df))\n",
    "\n",
    "# Split into train/test set\n",
    "train, test_template = split(\n",
    "    ds, offset=-TEST\n",
    ")  # assign last TEST time steps as test set\n",
    "\n",
    "\n",
    "\n",
    "# Construct rolling window evaluation\n",
    "test_data = test_template.generate_instances(\n",
    "    prediction_length=PDT,  # number of time steps for each prediction\n",
    "    windows=TEST // PDT,  # number of windows in rolling window evaluation\n",
    "    distance=PDT,  # number of time steps between each window - distance=PDT for non-overlapping windows\n",
    ")\n",
    "\n",
    "# Prepare pre-trained model by downloading model weights from huggingface hub\n",
    "model = MoiraiForecast(\n",
    "    module=MoiraiModule.from_pretrained(f\"Salesforce/moirai-1.0-R-{SIZE}\"),\n",
    "    prediction_length=PDT,\n",
    "    context_length=CTX,\n",
    "    patch_size=PSZ,\n",
    "    num_samples=100,\n",
    "    target_dim=1,\n",
    "    feat_dynamic_real_dim=ds.num_feat_dynamic_real,\n",
    "    past_feat_dynamic_real_dim=ds.num_past_feat_dynamic_real,\n",
    ")\n",
    "\n",
    "predictor = model.create_predictor(batch_size=BSZ)\n",
    "forecasts = predictor.predict(test_data.input)\n",
    "\n",
    "input_it = iter(test_data.input)\n",
    "label_it = iter(test_data.label)\n",
    "forecast_it = iter(forecasts)\n",
    "\n",
    "inp = next(input_it)\n",
    "label = next(label_it)\n",
    "forecast = next(forecast_it)\n",
    "\n",
    "plot_single(\n",
    "    inp, \n",
    "    label, \n",
    "    forecast, \n",
    "    context_length=200,\n",
    "    name=\"pred\",\n",
    "    show_label=True,\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install chronos-forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar modelo\n",
    "pipeline = BaseChronosPipeline.from_pretrained(\n",
    "    \"amazon/chronos-t5-small\",  # usa \"amazon/chronos-bolt-small\" para Chronos-Bolt\n",
    "    device_map=\"cpu\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Convertir datos (dfc y data_months deben estar definidos antes)\n",
    "df = dfc.convert_to_moirai(data_months)\n",
    "\n",
    "# --- 1. Usar solo datos hasta 2023 para entrenar el contexto ---\n",
    "df_context = df[df.index < \"2024-01-01\"]\n",
    "\n",
    "# Predecir 12 meses (todo 2024)\n",
    "quantiles, mean = pipeline.predict_quantiles(\n",
    "    context=torch.tensor(df_context[\"y\"].values, dtype=torch.float32).unsqueeze(0),\n",
    "    prediction_length=12,\n",
    "    quantile_levels=[0.1, 0.5, 0.9],\n",
    ")\n",
    "\n",
    "# --- 2. Construir índice de forecast para 2024 ---\n",
    "forecast_index = pd.date_range(start=\"2024-01-01\", periods=12, freq=\"MS\")\n",
    "\n",
    "low, median, high = quantiles[0, :, 0], quantiles[0, :, 1], quantiles[0, :, 2]\n",
    "\n",
    "# --- 3. Plotear ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Datos históricos completos (incluye 2024 real)\n",
    "plt.plot(df.index, df[\"y\"], color=\"royalblue\", label=\"historical data\")\n",
    "\n",
    "# Predicción (para 2024)\n",
    "plt.plot(forecast_index, median, color=\"tomato\", label=\"median forecast\")\n",
    "plt.fill_between(forecast_index, low, high, color=\"tomato\", alpha=0.3, label=\"80% prediction interval\")\n",
    "\n",
    "plt.axvline(pd.Timestamp(\"2024-01-01\"), color=\"gray\", linestyle=\"--\", label=\"forecast start\")\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Forecast vs Real Data (2024)\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a monthly time series dataset describing the price of Ethereum (ETH) in US dollars (USD). Each value represents the average price of Ethereum in USD for that month. from 736.22 increasing to 1313.8686084, from 1313.8686084 increasing to 1424.4, from 1424.4 increasing to 1919.3, from 1919.3 increasing to 2769.4, from 2769.4 decreasing to 2705.1, from 2705.1 decreasing to 2275.5, from 2275.5 increasing to 2530.0, from 2530.0 increasing to 3431.1, from 3431.1 decreasing to 3000.5, from 3000.5 increasing to 4288.3, from 4288.3 increasing to 4632.9, from 4632.9 decreasing to 3677.6, from 3677.6 decreasing to 2689.6, from 2689.6 increasing to 2925.3, from 2925.3 increasing to 3282.0, from 3282.0 decreasing to 2728.1, from 2728.1 decreasing to 1941.3, from 1941.3 decreasing to 1071.2, from 1071.2 increasing to 1677.7, from 1677.7 decreasing to 1554.4842756, from 1554.4842756 decreasing to 1329.2, from 1329.2 increasing to 1573.6, from 1573.6 decreasing to 1295.6, from 1295.6 decreasing to 1197.3, from 1197.3 increasing to 1585.6, from 1585.6 increasing to 1605.6, from 1605.6 increasing to 1821.6, from 1821.6 increasing to 1871.6, from 1871.6 increasing to 1874.6, from 1874.6 increasing to 1935.9, from 1935.9 decreasing to 1858.2, from 1858.2 decreasing to 1647.7, from 1647.7 increasing to 1671.9, from 1671.9 increasing to 1815.8, from 1815.8 increasing to 2052.7, \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nsteps = 12  # Número de meses a predecir\\nmodel_name = 'gpt-4o-mini'  \\npredicted_lan = mpt.paraphrasing_predict_llm(desp, train_lan, steps, model_name)\\nprint(predicted_lan)\\n# Convertir la predicción en lenguaje natural de vuelta a una serie temporal\\npredicted_series = mpt.recover_lan2seq(predicted_lan)\\n\\n# Mostrar la serie temporal predicha\\nprint(predicted_series)\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convertir la serie temporal a lenguaje natural\n",
    "desp = mpt.paraphrase_initial('ETH_MONTHS')\n",
    "train_lan = mpt.paraphrase_seq2lan(train_data_months, desp)\n",
    "print(train_lan)\n",
    "# Realizar la predicción para los próximos 12 meses (1 año)\n",
    "'''\n",
    "steps = 12  # Número de meses a predecir\n",
    "model_name = 'gpt-4o-mini'  \n",
    "predicted_lan = mpt.paraphrasing_predict_llm(desp, train_lan, steps, model_name)\n",
    "print(predicted_lan)\n",
    "# Convertir la predicción en lenguaje natural de vuelta a una serie temporal\n",
    "predicted_series = mpt.recover_lan2seq(predicted_lan)\n",
    "\n",
    "# Mostrar la serie temporal predicha\n",
    "print(predicted_series)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "# Asegúrate de haber iniciado sesión en Hugging Face si el modelo es privado\n",
    "# huggingface-cli login\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "prompt = \"\"\"\n",
    "<|begin_of_text|>\n",
    "Tengo una serie mensual de ventas en el siguiente formato:\n",
    "Enero 2023: 120\n",
    "Febrero 2023: 130\n",
    "Marzo 2023: 150\n",
    "Abril 2023: 170\n",
    "Mayo 2023: 160\n",
    "\n",
    "Predice los valores para Junio, Julio, Agosto, Septiembre, Octubre, Noviembre y Diciembre de 2023.\n",
    "Responde únicamente con los meses y los valores en el mismo formato.\n",
    "<|eot_id|>\n",
    "\"\"\"\n",
    "\n",
    "# Almacena el resultado en una variable\n",
    "outputs = pipeline(prompt)\n",
    "\n",
    "# El resultado es una lista de diccionarios, extrae el texto generado\n",
    "generated_text = outputs[0]['generated_text']\n",
    "\n",
    "# Imprime solo la parte de la predicción\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "# Usa un modelo más pequeño y eficiente como Gemma 2B\n",
    "model_id = \"google/gemma-2b-it\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_id, \n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16}, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Nuevo prompt para guiar mejor la respuesta\n",
    "prompt = \"\"\"\n",
    "<|start_of_turn|>user\n",
    "Tengo una serie mensual de ventas en el siguiente formato:\n",
    "Enero 2023: 120\n",
    "Febrero 2023: 130\n",
    "Marzo 2023: 150\n",
    "Abril 2023: 170\n",
    "Mayo 2023: 160\n",
    "\n",
    "Basado en estos datos, predice los valores de ventas para Junio, Julio y Agosto de 2023.\n",
    "Responde únicamente con una lista de los meses y sus valores.\n",
    "<|end_of_turn|>\n",
    "<|start_of_turn|>model\n",
    "\"\"\"\n",
    "\n",
    "# Almacena el resultado de la generación\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=2000, # Establece el número máximo de tokens a generar\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    pad_token_id=pipeline.tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Extrae solo la parte generada del texto\n",
    "# El resultado es una lista de diccionarios\n",
    "generated_text = outputs[0]['generated_text']\n",
    "\n",
    "# Elimina el prompt para ver solo la respuesta del modelo\n",
    "response = generated_text.replace(prompt, \"\")\n",
    "\n",
    "print(response.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Modelo de lenguaje pequeño y gratuito\n",
    "pipe = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "prompt = \"\"\"\n",
    "Tengo una serie mensual de ventas:\n",
    "Enero 2023: 120\n",
    "Febrero 2023: 130\n",
    "Marzo 2023: 150\n",
    "Abril 2023: 170\n",
    "Mayo 2023: 160\n",
    "\n",
    "Predice los valores de Junio a Diciembre 2023 en el mismo formato.\n",
    "\"\"\"\n",
    "\n",
    "response = pipe(prompt, max_new_tokens=100, do_sample=True)\n",
    "print(response[0][\"generated_text\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
