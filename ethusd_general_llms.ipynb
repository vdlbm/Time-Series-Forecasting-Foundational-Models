{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import my_data_frequency_conversor as dfc\n",
    "#import my_models as mm\n",
    "#import my_llm_ts_preprocessor as tsp\n",
    "import my_prompting_tool as mpt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an hourly time series dataset describing the price of Ethereum (ETH) in US dollars (USD), each value represents the average price of Ethereum in USD for that hour. \n",
      "This is a daily time series dataset describing the price of Ethereum (ETH) in US dollars (USD), each value represents the average price of Ethereum in USD for that day. \n",
      "This is a monthly time series dataset describing the price of Ethereum (ETH) in US dollars (USD), each value represents the average price of Ethereum in USD for that month. \n"
     ]
    }
   ],
   "source": [
    "# crear un diccionario con clave dataset y valor datos de training\n",
    "datasets = {\n",
    "    'ETH_HOURS':'DATA/ETHUSD-H1.csv',\n",
    "    'ETH_DAYS':'DATA/ETHUSD-D1.csv',\n",
    "    'ETH_MONTHS':'DATA/ETHUSD-Monthly.csv',\n",
    "    \n",
    "}\n",
    "freqs = {\n",
    "    'ETH_HOURS':'h',\n",
    "    'ETH_DAYS':'d',\n",
    "    'ETH_MONTHS':'MS'\n",
    "}\n",
    "\n",
    "descriptions = {\n",
    "    'ETH_HOURS':mpt.paraphrase_initial('ETH_HOURS'),\n",
    "    'ETH_DAYS':mpt.paraphrase_initial('ETH_DAYS'),\n",
    "    'ETH_MONTHS':mpt.paraphrase_initial('ETH_MONTHS')\n",
    "}\n",
    "train_data = {}\n",
    "val_data = {}\n",
    "for ds in datasets:\n",
    "    print(descriptions[ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETH_HOURS train data shape: (26280, 1)\n",
      "                           y\n",
      "DateTime                    \n",
      "2021-01-01 00:00:00  736.220\n",
      "2021-01-01 01:00:00  733.981\n",
      "2021-01-01 02:00:00  748.970\n",
      "2021-01-01 03:00:00  744.460\n",
      "2021-01-01 04:00:00  745.960\n",
      "ETH_HOURS val data shape: (8784, 1)\n",
      "                          y\n",
      "DateTime                   \n",
      "2024-01-01 00:00:00  2286.5\n",
      "2024-01-01 01:00:00  2295.6\n",
      "2024-01-01 02:00:00  2305.4\n",
      "2024-01-01 03:00:00  2295.8\n",
      "2024-01-01 04:00:00  2275.4\n",
      "ETH_DAYS train data shape: (1095, 1)\n",
      "                  y\n",
      "DateTime           \n",
      "2021-01-01   736.22\n",
      "2021-01-02   729.46\n",
      "2021-01-03   774.82\n",
      "2021-01-04   976.32\n",
      "2021-01-05  1041.20\n",
      "ETH_DAYS val data shape: (366, 1)\n",
      "                 y\n",
      "DateTime          \n",
      "2024-01-01  2286.5\n",
      "2024-01-02  2352.7\n",
      "2024-01-03  2358.2\n",
      "2024-01-04  2211.3\n",
      "2024-01-05  2271.0\n",
      "ETH_MONTHS train data shape: (36, 1)\n",
      "                   y\n",
      "DateTime            \n",
      "2021-01-01   736.220\n",
      "2021-02-01  1313.869\n",
      "2021-03-01  1424.400\n",
      "2021-04-01  1919.300\n",
      "2021-05-01  2769.400\n",
      "ETH_MONTHS val data shape: (12, 1)\n",
      "                 y\n",
      "DateTime          \n",
      "2024-01-01  2286.5\n",
      "2024-02-01  2283.0\n",
      "2024-03-01  3339.2\n",
      "2024-04-01  3650.6\n",
      "2024-05-01  3020.3\n"
     ]
    }
   ],
   "source": [
    "# Itera sobre cada conjunto de datos en el diccionario 'datasets'\n",
    "for ds_name in datasets:  \n",
    "    # Carga el archivo CSV correspondiente en un DataFrame de pandas\n",
    "    df = pd.read_csv(datasets[ds_name])  \n",
    "    \n",
    "    # Divide los datos en entrenamiento y validación usando la función split_data de dfc\n",
    "    train_data[ds_name], val_data[ds_name] = dfc.split_data(df)  \n",
    "    \n",
    "    # Ajusta la frecuencia de los datos de entrenamiento según el diccionario 'freqs'\n",
    "    train_data[ds_name] = train_data[ds_name].asfreq(freqs[ds_name])  \n",
    "    \n",
    "    # Ajusta la frecuencia de los datos de validación según el diccionario 'freqs'\n",
    "    val_data[ds_name] = val_data[ds_name].asfreq(freqs[ds_name])  \n",
    "    \n",
    "    # Redondea los valores de los datos de entrenamiento a 3 decimales\n",
    "    train_data[ds_name] = train_data[ds_name].round(3)  \n",
    "    \n",
    "    # Redondea los valores de los datos de validación a 3 decimales\n",
    "    val_data[ds_name] = val_data[ds_name].round(3)  \n",
    "    \n",
    "    # Imprime la forma (cantidad de filas y columnas) del conjunto de entrenamiento\n",
    "    print(f'{ds_name} train data shape: {train_data[ds_name].shape}')  \n",
    "    \n",
    "    # Muestra las primeras filas del conjunto de entrenamiento\n",
    "    print(train_data[ds_name].head())  \n",
    "    \n",
    "    # Imprime la forma del conjunto de validación\n",
    "    print(f'{ds_name} val data shape: {val_data[ds_name].shape}')  \n",
    "    \n",
    "    # Muestra las primeras filas del conjunto de validación\n",
    "    print(val_data[ds_name].head())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Processing ETH_HOURS dataset\n",
      "Longitud del entrenamiento: (26280, 1)\n",
      "Longitud de la prueba: (8784, 1)\n",
      "NaN detectado en: y    2137.31\n",
      "Name: 2021-06-30 07:00:00, dtype: float64 y   NaN\n",
      "Name: 2021-06-30 08:00:00, dtype: float64\n",
      "NaN detectado en: y   NaN\n",
      "Name: 2021-06-30 08:00:00, dtype: float64 y   NaN\n",
      "Name: 2021-06-30 09:00:00, dtype: float64\n",
      "NaN detectado en: y   NaN\n",
      "Name: 2021-06-30 09:00:00, dtype: float64 y   NaN\n",
      "Name: 2021-06-30 10:00:00, dtype: float64\n",
      "NaN detectado en: y   NaN\n",
      "Name: 2021-06-30 10:00:00, dtype: float64 y   NaN\n",
      "Name: 2021-06-30 11:00:00, dtype: float64\n",
      "NaN detectado en: y   NaN\n",
      "Name: 2021-06-30 11:00:00, dtype: float64 y    2168.0\n",
      "Name: 2021-06-30 12:00:00, dtype: float64\n",
      "NaN detectado en: y    2168.0\n",
      "Name: 2021-06-30 12:00:00, dtype: float64 y   NaN\n",
      "Name: 2021-06-30 13:00:00, dtype: float64\n",
      "NaN detectado en: y   NaN\n",
      "Name: 2021-06-30 13:00:00, dtype: float64 y    2168.0\n",
      "Name: 2021-06-30 14:00:00, dtype: float64\n",
      "NaN detectado en: y    2168.0\n",
      "Name: 2021-06-30 14:00:00, dtype: float64 y   NaN\n",
      "Name: 2021-06-30 15:00:00, dtype: float64\n",
      "NaN detectado en: y   NaN\n",
      "Name: 2021-06-30 15:00:00, dtype: float64 y   NaN\n",
      "Name: 2021-06-30 16:00:00, dtype: float64\n",
      "NaN detectado en: y   NaN\n",
      "Name: 2021-06-30 16:00:00, dtype: float64 y   NaN\n",
      "Name: 2021-06-30 17:00:00, dtype: float64\n",
      "NaN detectado en: y   NaN\n",
      "Name: 2021-06-30 17:00:00, dtype: float64 y    2140.1\n",
      "Name: 2021-06-30 18:00:00, dtype: float64\n",
      "NaN detectado en: y    3451.308\n",
      "Name: 2021-10-13 10:00:00, dtype: float64 y   NaN\n",
      "Name: 2021-10-13 11:00:00, dtype: float64\n",
      "NaN detectado en: y   NaN\n",
      "Name: 2021-10-13 11:00:00, dtype: float64 y    3442.3\n",
      "Name: 2021-10-13 12:00:00, dtype: float64\n",
      "NaN detectado en: y    3442.3\n",
      "Name: 2021-10-13 12:00:00, dtype: float64 y   NaN\n",
      "Name: 2021-10-13 13:00:00, dtype: float64\n",
      "NaN detectado en: y   NaN\n",
      "Name: 2021-10-13 13:00:00, dtype: float64 y   NaN\n",
      "Name: 2021-10-13 14:00:00, dtype: float64\n",
      "NaN detectado en: y   NaN\n",
      "Name: 2021-10-13 14:00:00, dtype: float64 y   NaN\n",
      "Name: 2021-10-13 15:00:00, dtype: float64\n",
      "NaN detectado en: y   NaN\n",
      "Name: 2021-10-13 15:00:00, dtype: float64 y   NaN\n",
      "Name: 2021-10-13 16:00:00, dtype: float64\n",
      "NaN detectado en: y   NaN\n",
      "Name: 2021-10-13 16:00:00, dtype: float64 y    3456.679\n",
      "Name: 2021-10-13 17:00:00, dtype: float64\n",
      "NaN detectado en: y    1297.328\n",
      "Name: 2022-10-12 07:00:00, dtype: float64 y   NaN\n",
      "Name: 2022-10-12 08:00:00, dtype: float64\n",
      "NaN detectado en: y   NaN\n",
      "Name: 2022-10-12 08:00:00, dtype: float64 y   NaN\n",
      "Name: 2022-10-12 09:00:00, dtype: float64\n",
      "NaN detectado en: y   NaN\n",
      "Name: 2022-10-12 09:00:00, dtype: float64 y   NaN\n",
      "Name: 2022-10-12 10:00:00, dtype: float64\n",
      "NaN detectado en: y   NaN\n",
      "Name: 2022-10-12 10:00:00, dtype: float64 y    1294.1\n",
      "Name: 2022-10-12 11:00:00, dtype: float64\n",
      "NaN detectado en: y    1564.2\n",
      "Name: 2023-03-06 09:00:00, dtype: float64 y   NaN\n",
      "Name: 2023-03-06 10:00:00, dtype: float64\n",
      "NaN detectado en: y   NaN\n",
      "Name: 2023-03-06 10:00:00, dtype: float64 y   NaN\n",
      "Name: 2023-03-06 11:00:00, dtype: float64\n",
      "NaN detectado en: y   NaN\n",
      "Name: 2023-03-06 11:00:00, dtype: float64 y    1564.3\n",
      "Name: 2023-03-06 12:00:00, dtype: float64\n",
      "NaN detectado en: y    3511.3\n",
      "Name: 2024-03-30 00:00:00, dtype: float64 y   NaN\n",
      "Name: 2024-03-30 01:00:00, dtype: float64\n",
      "NaN detectado en: y   NaN\n",
      "Name: 2024-03-30 01:00:00, dtype: float64 y    3508.3\n",
      "Name: 2024-03-30 02:00:00, dtype: float64\n",
      "NaN detectado en: y    3162.6\n",
      "Name: 2024-05-05 09:00:00, dtype: float64 y   NaN\n",
      "Name: 2024-05-05 10:00:00, dtype: float64\n",
      "NaN detectado en: y   NaN\n",
      "Name: 2024-05-05 10:00:00, dtype: float64 y   NaN\n",
      "Name: 2024-05-05 11:00:00, dtype: float64\n",
      "NaN detectado en: y   NaN\n",
      "Name: 2024-05-05 11:00:00, dtype: float64 y    3145.6\n",
      "Name: 2024-05-05 12:00:00, dtype: float64\n",
      "Longitud de la secuencia predicha: (8779, 1)\n",
      "Warning! Se perdieron datos.\n",
      "        0\n",
      "0  2286.5\n",
      "1  2295.6\n",
      "2  2305.4\n",
      "3  2295.8\n",
      "4  2275.4\n",
      "\n",
      "--------------------------------------------------\n",
      "Processing ETH_DAYS dataset\n",
      "Longitud del entrenamiento: (1095, 1)\n",
      "Longitud de la prueba: (366, 1)\n",
      "Longitud de la secuencia predicha: (366, 1)\n",
      "\n",
      "--------------------------------------------------\n",
      "Processing ETH_MONTHS dataset\n",
      "Longitud del entrenamiento: (36, 1)\n",
      "Longitud de la prueba: (12, 1)\n",
      "Longitud de la secuencia predicha: (12, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    print(\"-\" * 50)  # Línea divisoria\n",
    "    print(f'Processing {dataset} dataset')\n",
    "    train, test, seq = mpt.paraphrase_nlp(dataset, train_data[dataset], val_data[dataset])\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "PROMPTS GENERALES EN LOS INPUTS:\n",
      "--------------------\n",
      "Número de tokens del prompt inicial: 65\n",
      "Número de tokens del inicio de la descripción: 56\n",
      "--------------------------------------------------\n",
      "PROMPTS ESPECÍFICOS DE CADA FRECUENCIA:\n",
      "--------------------\n",
      "Processing ETH_HOURS dataset\n",
      "Número de tokens solo de la serie temporal en input: 368101\n",
      "Número de tokens totales del input ETH_HOURS: 368222\n",
      "Número de tokens del output ETH_HOURS: 123027\n",
      "--------------------------------------------------\n",
      "Processing ETH_DAYS dataset\n",
      "Número de tokens solo de la serie temporal en input: 15347\n",
      "Número de tokens totales del input ETH_DAYS: 15468\n",
      "Número de tokens del output ETH_DAYS: 5111\n",
      "--------------------------------------------------\n",
      "Processing ETH_MONTHS dataset\n",
      "Número de tokens solo de la serie temporal en input: 525\n",
      "Número de tokens totales del input ETH_MONTHS: 646\n",
      "Número de tokens del output ETH_MONTHS: 155\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# contar el número de tokens de args\n",
    "import tiktoken as tk\n",
    "\n",
    "encoding = tk.get_encoding(\"cl100k_base\")  # Compatible con GPT-4 y GPT-3.5-turbo\n",
    "\n",
    "print (\"-\" * 50)  # Línea divisoria\n",
    "print(\"PROMPTS GENERALES EN LOS INPUTS:\")\n",
    "print (\"-\" * 20)  # Línea divisoria\n",
    "\n",
    "# Contar tokens de prompt inicial\n",
    "prompt_ini = \"You are a helpful assistant that performs time series predictions. \" \\\n",
    "             \"The user will provide a sequence and you will predict the remaining sequence.\" \\\n",
    "             \"The sequence is represented by decimal strings separated by commas. \" \\\n",
    "             \"Please continue the following sequence without producing any additional text. \" \\\n",
    "             \"Do not say anything like 'the next terms in the sequence are', just return the numbers.\"\n",
    "\n",
    "num_tokens_prompt_ini = len(encoding.encode(prompt_ini))\n",
    "print(f\"Número de tokens del prompt inicial: {num_tokens_prompt_ini}\")\n",
    "\n",
    "ini_desc = \"Predict the next 12 steps, where each step follows the format (from 1.0 increasing to 2.0) or (from 2.0 decreasing to 0.5).\"\\\n",
    "                  \" The final output should precisely follow the specified number of steps. Provide a sequence:\\n\"\n",
    "num_tokens_ini_desc = len(encoding.encode(ini_desc))\n",
    "print(f\"Número de tokens del inicio de la descripción: {num_tokens_ini_desc}\")\n",
    "\n",
    "print (\"-\" * 50)  # Línea divisoria\n",
    "print(\"PROMPTS ESPECÍFICOS DE CADA FRECUENCIA:\")\n",
    "print (\"-\" * 20)  # Línea divisoria\n",
    "\n",
    "# Contar tokens de las secuencias:\n",
    "for ds in datasets:\n",
    "    print(f'Processing {ds} dataset')\n",
    "    \n",
    "    # Sustituyo nans, ya que solo quiero contar tokens\n",
    "    new_train_data = train_data[ds].fillna(train_data[ds].mean())\n",
    "    new_val_data = val_data[ds].fillna(val_data[ds].mean())\n",
    "\n",
    "    # Secuencia de la serie temporal\n",
    "    input_seq = mpt.paraphrase_seq2lan(new_train_data, descriptions[ds])\n",
    "    num_tokens_input = len(encoding.encode(input_seq))\n",
    "    print(f\"Número de tokens solo de la serie temporal en input: {num_tokens_input}\")\n",
    "\n",
    "    # Input completo\n",
    "    full_input = prompt_ini + ini_desc + input_seq\n",
    "    num_tokens_full_input = len(encoding.encode(full_input))\n",
    "    print(f\"Número de tokens totales del input {ds}: {num_tokens_full_input}\")\n",
    "\n",
    "    # Output\n",
    "    output_seq = mpt.paraphrase_seq2lan(new_val_data, '')\n",
    "    num_tokens_output = len(encoding.encode(output_seq))\n",
    "    print(f\"Número de tokens del output {ds}: {num_tokens_output}\")\n",
    "    print (\"-\" * 50)  # Línea divisoria\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
